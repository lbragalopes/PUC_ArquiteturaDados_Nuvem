{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657e0dac-8ddf-4b30-949c-06e7ad3fc47a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "spark = SparkSession.builder.appName(\"read_csv_from_s3\").getOrCreate()\n",
    "\n",
    "s3_bucket_name = \"projeto-puc\"\n",
    "s3_folder_path = \"raw_db/\"\n",
    "\n",
    "# Listar todos os arquivos do S3\n",
    "s3_files = []\n",
    "for s3_obj in boto3.resource('s3').Bucket(s3_bucket_name).objects.filter(Prefix=s3_folder_path):\n",
    "    if s3_obj.key.endswith('.csv'):\n",
    "        s3_files.append(\"s3://\" + s3_bucket_name + \"/\" + s3_obj.key)\n",
    "\n",
    "for s3_file in s3_files:\n",
    "    df = spark.read.csv(s3_file, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "    # Colunas incomuns que deverão ser excluídas\n",
    "    cols_to_drop = ['Gestor','Canal de Origem','Ano Abertura','Mês Abertura','Data Abertura','Hora Abertura',\t'Data Resposta','Hora Resposta','Data Análise','Hora Análise','Data Recusa','Hora Recusa','Hora Finalização','Prazo Resposta','Prazo Analise Gestor','Contratou','Nota do Consumidor','Análise da Recusa','Edição de Conteúdo','Interação do Gestor','Total']\n",
    "\n",
    "    # Verifica se existe colunas em todos arquivos e remove as colunas listadas\n",
    "    if cols_to_drop:    \n",
    "        for col in cols_to_drop:\n",
    "            df = df.drop(col)\n",
    "\n",
    "    # Remove linhas duplicadas\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Remover linhas em branco\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Após as transformações salva os arquivos na pasta ref_db no S3\n",
    "    output_filename = s3_file.split(\"/\")[-1]  \n",
    "    df.write.mode(\"overwrite\").csv(\"s3://\" + s3_bucket_name + \"/ref_db/\" + output_filename, header=True, sep=\";\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "processamento",
   "notebookOrigID": 3251792637263944,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aeee767de3ff3670abb362b2e0e4fa4a1c7700662b2e8cbafa506171ad5999cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
