{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1507f4d0-fefd-4f46-a2cd-481a99ec13fe","showTitle":false,"title":""}},"outputs":[],"source":["import mlflow\n","import mlflow.spark\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.sql.functions import isnan, when, count, col\n","from pyspark.sql.functions import regexp_extract\n","from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"72be0e3a-972d-4d09-931d-9f1547be401d","showTitle":false,"title":""}},"outputs":[],"source":["# Lê todos os arquivos parquet no bucket S3 e visualiza o DataFrame. \n","file_type = \"parquet\"\n","infer_schema = \"false\"\n","first_row_is_header = \"true\"\n","delimiter = \",\"\n","s3_bucket_name = \"projeto-puc\"\n","s3_folder_path = \"ref_db/\"\n","\n","df = spark.read.format(file_type) \\\n","  .option(\"inferSchema\", infer_schema) \\\n","  .option(\"header\", first_row_is_header) \\\n","  .option(\"sep\", delimiter) \\\n","  .load(f\"s3://{s3_bucket_name}/{s3_folder_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a1489e46-d01b-4716-9d9a-30747ab3fb02","showTitle":false,"title":""}},"outputs":[],"source":["# verificando a qualidade dos dados e anomalias da coluna Tempo_Resposta\n","tempo_resposta_counts = df.groupBy(\"Tempo_Resposta\").agg(count(\"*\").alias(\"Quantidade_registros\")).orderBy(\"Quantidade_registros\")\n","\n","# excluindo os dados da coluna Tempo_Reposta com anomalias.\n","tempo_resposta_counts_filtered = tempo_resposta_counts.filter(\"Quantidade_registros > 1\")\n","df = df.filter(df[\"Tempo_Resposta\"].isin(tempo_resposta_counts_filtered.select(\"Tempo_Resposta\").rdd.flatMap(lambda x: x).collect()))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"5f15fcff-00ce-4f05-90c1-d65783091e41","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+---------------+-----------------+\n","|Tempo_Resposta|Nota_Consumidor|Comprou_Contratou|\n","+--------------+---------------+-----------------+\n","|             0|              0|                0|\n","+--------------+---------------+-----------------+\n","\n"]}],"source":["#verificando dados nulos ou NaN\n","df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"Tempo_Resposta\", \"Nota_Consumidor\", \"Comprou_Contratou\"]]).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"da004249-4e47-470a-90df-5973a3f2cab2","showTitle":false,"title":""}},"outputs":[],"source":["# Transforma a coluna \"Comprou_Contratou\" em numérica\n","# Define um dicionário com as categorias e seus respectivos números sequenciais\n","\n","categorias = {\n","    \"Stand, feiras e eventos\": 0,\n","    \"Catálogo\": 1,\n","    \"Ganhei de presente\": 2,\n","    \"SMS / Mensagem de texto\": 3,\n","    \"Domicílio\": 4,\n","    \"Telefone\": 5,\n","    \"Loja física\": 6,\n","    \"Contratei\": 7,\n","    \"Internet\": 8\n","}\n","\n","df = df.withColumn(\"Comprou_Contratou\", when(col(\"Comprou_Contratou\") == \"Sim\", 1).otherwise(0))\n","for categoria, numero in categorias.items():\n","    df = df.withColumn(\"Categoria_\" + str(numero), when(col(\"Comprou_Contratou\") == categoria, 1).otherwise(0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"0e87a99c-8baa-4210-9a3a-e54cab6fbdb6","showTitle":false,"title":""}},"outputs":[],"source":["# Conversão do tipo string para interger na coluna Tempo_Resposta\n","df = df.withColumn('Tempo_Resposta', regexp_extract(df['Tempo_Resposta'], '\\d+', 0).cast('integer'))\n","df = df.withColumn('Nota_Consumidor', regexp_extract(df['Nota_Consumidor'], '\\d+', 0).cast('integer'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"4d9005f4-a390-44a0-ad78-39c8baa54280","showTitle":false,"title":""}},"outputs":[],"source":["# Seleciona as colunas relevantes e filtra apenas as situacoes finalizadas\n","data = df.select(\"Tempo_Resposta\", *[\"Categoria_\" + str(numero) for numero in range(len(categorias))], \"Nota_Consumidor\").filter(col(\"Situação\") == \"Finalizada avaliada\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1d2d0f29-934a-40fa-bdc0-ccc65c9273f1","showTitle":false,"title":""}},"source":["#### Radom Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1e4b930e-5cd5-4c29-bb6e-c7a6ae5787cc","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE: 3.067\n","RMSE: 1.751\n"]},{"name":"stderr","output_type":"stream","text":["2023/05/14 21:44:44 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n","/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n","  warnings.warn(\"Setuptools is replacing distutils.\")\n"]}],"source":["# Define as informações do experimento e inicia o MLflow\n","mlflow.set_experiment(\"/Users/lbragalopes@gmail.com/ProjetoPuc\")\n","mlflow.start_run()\n","\n","# Transforma as colunas em um vetor de features\n","assembler = VectorAssembler(inputCols=[\"Tempo_Resposta\", *[\"Categoria_\" + str(numero) for numero in range(len(categorias))]], outputCol=\"features\")\n","\n","# Cria o modelo de regressão\n","rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Nota_Consumidor\", numTrees=10, maxDepth=5)\n","\n","# Cria o pipelines com o assembler e cada modelo\n","rf_pipeline = Pipeline(stages=[assembler, rf])\n","\n","\n","# Divide os dados em treino e teste\n","(trainingData, testData) = data.randomSplit([0.7, 0.3])\n","\n","# Treina o modelo\n","rf_model = rf_pipeline.fit(trainingData)\n","\n","# Faz as previsões no conjunto de teste\n","rf_predictions = rf_model.transform(testData)\n","\n","# Avalia o modelo com as métricas MSE e RMSE\n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"mse\")\n","rf_mse = evaluator.evaluate(rf_predictions)\n","print(\"MSE: %.3f\" % rf_mse)\n","\n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"rmse\")\n","rf_rmse = evaluator.evaluate(rf_predictions)\n","print(\"RMSE: %.3f\" % rf_rmse)\n","\n","# Loga os parâmetros e métricas no MLflow\n","mlflow.log_param(\"rf_numTrees\", rf.getNumTrees())\n","mlflow.log_param(\"rf_maxDepth\", rf.getMaxDepth())\n","mlflow.log_metric(\"rf_mse\", rf_mse)\n","mlflow.log_metric(\"rf_rmse\", rf_rmse)\n","\n","# Salva o modelo no MLflow\n","mlflow.spark.log_model(rf_model, \"random_forest\")\n","\n","# Finaliza o MLflow\n","mlflow.end_run()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"8a7bf02a-5395-42f4-abe4-b6cff3dcc0a1","showTitle":false,"title":""}},"source":["#### Regression Linear"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e62b147e-95fd-4ba4-ab43-3abc76f41502","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE: 3.091\n","RMSE: 1.758\n"]},{"name":"stderr","output_type":"stream","text":["2023/05/14 21:49:08 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"]}],"source":["# Define as informações do experimento e inicia o MLflow\n","mlflow.set_experiment(\"/Users/lbragalopes@gmail.com/ProjetoPuc\")\n","mlflow.start_run()\n","\n","# Transforma as colunas em um vetor de features\n","assembler = VectorAssembler(inputCols=[\"Tempo_Resposta\", *[\"Categoria_\" + str(numero) for numero in range(len(categorias))]], outputCol=\"features\")\n","\n","# Cria o modelo de regressão\n","lr = LinearRegression(featuresCol=\"features\", labelCol=\"Nota_Consumidor\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n","\n","# Cria os pipelines com o assembler e cada modelo\n","lr_pipeline = Pipeline(stages=[assembler, lr])\n","\n","# Divide os dados em treino e teste\n","(trainingData, testData) = data.randomSplit([0.7, 0.3])\n","\n","# Treina o modelo\n","lr_model = lr_pipeline.fit(trainingData)\n","\n","# Faz as previsões no conjunto de teste\n","lr_predictions = lr_model.transform(testData)\n","\n","# Avalia o modelo com as métricas MSE \n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"mse\")\n","lr_mse = evaluator.evaluate(lr_predictions)\n","print(\"MSE: %.3f\" % lr_mse)\n","\n","# Avalia o modelo com as métricas RMSE \n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"rmse\")\n","lr_rmse = evaluator.evaluate(lr_predictions)\n","print(\"RMSE: %.3f\" % lr_rmse)\n","\n","# Loga os parâmetros e métricas no MLflow\n","mlflow.log_param(\"lr_maxIter\", lr.getMaxIter())\n","mlflow.log_param(\"lr_regParam\", lr.getRegParam())\n","mlflow.log_param(\"lr_elasticNetParam\", lr.getElasticNetParam())\n","mlflow.log_metric(\"lr_mse\", lr_mse)\n","mlflow.log_metric(\"lr_rmse\", lr_rmse)\n","\n","# Salva o modelo no MLflow\n","mlflow.spark.log_model(lr_model, \"regressao_linear\")\n","\n","# Finaliza o MLflow\n","mlflow.end_run()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c572757a-bb0f-4ae0-9855-e10058c75c4b","showTitle":false,"title":""}},"source":["#### Gradient Boosting Machine (GBM)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3f7fb031-cbf6-4e54-a4c4-ab35a9794957","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE: 3.044\n","MSE: 1.745\n"]},{"name":"stderr","output_type":"stream","text":["2023/05/14 21:55:47 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"]}],"source":["mlflow.set_experiment(\"/Users/lbragalopes@gmail.com/ProjetoPuc\")\n","mlflow.start_run()\n","\n","# Transforma as colunas em um vetor de features\n","assembler = VectorAssembler(inputCols=[\"Tempo_Resposta\", *[\"Categoria_\" + str(numero) for numero in range(len(categorias))]], outputCol=\"features\")\n","\n","# Cria o modelo de regressão\n","gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Nota_Consumidor\", maxIter=10, maxDepth=5)\n","\n","# Cria os pipelines com o assembler e cada modelo\n","gbt_pipeline = Pipeline(stages=[assembler, gbt])\n","\n","# Divide os dados em treino e teste\n","(trainingData, testData) = data.randomSplit([0.7, 0.3])\n","\n","# Treina o modelo\n","gbt_model = gbt_pipeline.fit(trainingData)\n","\n","# Faz as previsões no conjunto de teste\n","gbt_predictions = gbt_model.transform(testData)\n","\n","# Avalia os modelos com as métricas MSE e RMSE\n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"mse\")\n","gbt_mse = evaluator.evaluate(gbt_predictions)\n","print(\"MSE: %.3f\" % gbt_mse)\n","\n","evaluator = RegressionEvaluator(labelCol=\"Nota_Consumidor\", predictionCol=\"prediction\", metricName=\"rmse\")\n","gbt_rmse = evaluator.evaluate(gbt_predictions)\n","print(\"MSE: %.3f\" % gbt_rmse)\n","\n","# Loga os parâmetros e métricas no MLflow\n","mlflow.log_param(\"gbt_maxIter\", gbt.getMaxIter())\n","mlflow.log_param(\"gbt_regParam\", gbt.getMaxDepth())\n","mlflow.log_metric(\"gbt_mse\", gbt_mse)\n","mlflow.log_metric(\"gbt_rmse\", gbt_rmse)\n","\n","# Salva o modelo no MLflow\n","mlflow.spark.log_model(gbt_model, \"gbt\")\n","\n","# Finaliza o MLflow\n","mlflow.end_run()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Regressao_ML","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
